% ============================================================================
% DISCUSSION
% ============================================================================
\section{Discussion}
\label{sec:discussion}

\sloppy

The experimental results reveal significant insights about the relative effectiveness of supervised, semi-supervised, and self-supervised learning paradigms for brain MRI object detection. DINOv3 + YOLO achieved the highest mAP@0.5 of 94.08\%, surpassing all other configurations including the fully supervised baseline.

\subsection{Baseline Model Analysis}

The progression from YOLOv10n to YOLOv12n demonstrated consistent improvements in detection performance. YOLOv12n's superior performance (88.54\% mAP@0.5 vs. 81.36\% for YOLOv10n) can be attributed to several architectural advancements:

\begin{itemize}
    \item \textbf{Attention Mechanisms:} YOLOv12 incorporates transformer-like attention modules that better capture long-range dependencies in medical images.
    \item \textbf{Improved Feature Aggregation:} Enhanced neck architectures facilitate better multi-scale feature fusion.
    \item \textbf{Refined Detection Heads:} More sophisticated prediction heads improve localization precision.
\end{itemize}

The relatively high performance across all models (>80\% mAP@0.5) suggests that modern YOLO architectures are well-suited for brain MRI pathology detection, even without domain-specific modifications.

\subsection{Semi-Supervised Learning Analysis}

The teacher model trained on only 20\% labeled data achieved 81.84\% mAP@0.5, representing an 11.2\% performance drop from the full baseline (93.04\%). This performance gap is expected given the significant reduction in labeled training data. Contrary to expectations, the student model trained on pseudo-labeled data performed worse than the teacher (73.66\% vs. 81.84\%). This unexpected result can be attributed to:

\begin{enumerate}
    \item \textbf{Pseudo-Label Noise:} Despite the 0.70 confidence threshold, noisy pseudo-labels introduced erroneous supervision signals.
    \item \textbf{Class Imbalance:} The pseudo-labeling process disproportionately generated labels for easier classes.
    \item \textbf{Confirmation Bias:} The student reinforced teacher errors rather than correcting them.
    \item \textbf{Distribution Shift:} Images with confident pseudo-labels may not represent the full data distribution.
\end{enumerate}

These results highlight that simple pseudo-labeling requires careful implementation. More sophisticated approaches such as Unbiased Teacher \citep{liu2021unbiased} or curriculum-based pseudo-labeling may be necessary for positive transfer from unlabeled data.

\subsection{Self-Supervised Learning Analysis}

SimCLR achieved 58.59\% linear evaluation accuracy, demonstrating that contrastive pretraining learns task-relevant representations. The improvement to 90.31\% after full fine-tuning validates the transfer learning paradigm. DINOv3 features exhibited remarkable properties: (1) strong zero-shot clustering with clear class separation in t-SNE visualizations; (2) high linear probe accuracy (85.23\%) significantly outperforming SimCLR; and (3) effective transfer to detection, achieving 94.08\% mAP@0.5 when integrated with YOLOv12.

\subsection{Ablation Study}

Table \ref{tab:ablation} presents a comprehensive ablation study comparing all model configurations across multiple metrics.

\begin{table}[htbp]
\centering
\caption{Ablation Study: Comprehensive Model Comparison}
\label{tab:ablation}
\footnotesize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{@{}llcccccc@{}}
\toprule
\textbf{Type} & \textbf{Model} & \textbf{mAP50} & \textbf{mAP50-95} & \textbf{Prec.} & \textbf{Rec.} & \textbf{F1} & \textbf{$\Delta$} \\
\midrule
\multirow{3}{*}{Sup.} 
 & YOLOv10n & 81.36 & 55.39 & 79.36 & 67.82 & 73.14 & -- \\
 & YOLOv11n & 84.47 & 58.29 & 72.44 & 80.48 & 76.25 & +3.11 \\
 & YOLOv12n & 88.54 & 60.32 & 83.20 & 79.50 & 81.31 & +7.18 \\
\midrule
\multirow{3}{*}{Semi}
 & Base (100\%) & 93.04 & 64.59 & 84.66 & 86.55 & 85.59 & +11.68 \\
 & Teacher (20\%) & 81.84 & 53.92 & 72.11 & 79.34 & 75.55 & +0.48 \\
 & Student & 73.66 & 49.55 & 71.19 & 69.08 & 70.12 & -7.70 \\
\midrule
\multirow{4}{*}{Self}
 & SimCLR Lin. & 58.59 & -- & 56.81 & 58.59 & 54.60 & -22.77 \\
 & SimCLR Fine & 90.31 & -- & 90.33 & 90.31 & 90.31 & +8.95 \\
 & SimCLR+YOLO & 91.89 & 65.09 & 82.55 & 84.45 & 83.49 & +10.53 \\
 & \textbf{DINOv3+YOLO} & \textbf{94.08} & \textbf{67.73} & \textbf{86.33} & \textbf{89.49} & \textbf{87.88} & \textbf{+12.72} \\
\bottomrule
\end{tabular}
\end{table}

Key findings from the ablation study:
\begin{itemize}
    \item \textbf{Architecture Impact:} Upgrading from YOLOv10n to YOLOv12n improves mAP@50 by 7.18\%.
    \item \textbf{SSL Limitations:} Naive pseudo-labeling (Student) degrades performance by 7.70\% relative to baseline.
    \item \textbf{Self-Supervised Superiority:} DINOv3+YOLO achieves the highest improvement (+12.72\%) over baseline.
    \item \textbf{Fine-tuning Essential:} SimCLR Linear (58.59\%) vs Fine-tuned (90.31\%) shows +31.72\% improvement.
\end{itemize}

\subsection{Computational Comparison}

Table \ref{tab:compute_comparison} compares the computational requirements of each method.

\begin{table}[htbp]
\centering
\caption{Computational Comparison of Methods}
\label{tab:compute_comparison}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Method} & \textbf{Pretraining} & \textbf{Fine-tuning} & \textbf{Labeled Data} & \textbf{GPU Hours} \\
\midrule
YOLOv12n Baseline & None & 100 epochs & 100\% & $\sim$2h \\
SSOD Teacher-Student & None & 200 epochs & 20\% & $\sim$4h \\
SimCLR + YOLO & 100 epochs & 50 epochs & 100\% & $\sim$6h \\
DINOv3 + YOLO & Pretrained & 20 epochs & 100\% & $\sim$1.5h \\
\bottomrule
\end{tabular}
\end{table}

DINOv3's advantage includes using publicly available pretrained weights, eliminating expensive self-supervised pretraining on the target domain while achieving the best performance.

\subsection{Lessons Learned}

\begin{enumerate}
    \item \textbf{Pretrained Transformers Excel:} Vision Transformer models pretrained with self-distillation (DINOv3) provide superior features for medical imaging compared to CNN-based contrastive learning (SimCLR).
    
    \item \textbf{Simple SSL Has Limitations:} Naive pseudo-labeling without additional regularization mechanisms can harm rather than help performance.
    
    \item \textbf{Transfer Learning is Powerful:} Leveraging large-scale pretrained models (even from non-medical domains) outperforms fully supervised training on domain-specific data.
    
    \item \textbf{Architecture Matters:} Progressive improvements in YOLO architectures translate directly to better detection in medical imaging.
\end{enumerate}

\subsection{Limitations}

The study has several limitations: (1) the relatively small dataset ($\sim$1,200 images) may limit generalizability; (2) results are specific to brain MRI and may not generalize to other modalities; (3) only simple pseudo-labeling was evaluated for SSL; and (4) experiments were limited by Kaggle GPU quotas.
