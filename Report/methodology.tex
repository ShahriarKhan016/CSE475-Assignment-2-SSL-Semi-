% ============================================================================
% METHODOLOGY
% ============================================================================
\section{Methodology}
\label{sec:methodology}

\sloppy

The proposed methodology comprises three main components: (1) supervised baseline using YOLO architectures, (2) semi-supervised detection with pseudo-labeling, and (3) self-supervised representation learning with SimCLR and DINOv3. Each approach is evaluated on the Brain MRI dataset for pathology detection.

\subsection{Dataset}

\subsubsection{Dataset Description}

The Brain MRI object detection dataset\footnote{Dataset available at: \url{https://www.kaggle.com/datasets/turjo410/brain-mri-split-dataset}} contains approximately 1,200 high-resolution MRI scans with expert bounding box annotations. Table \ref{tab:dataset} summarizes the dataset characteristics. The images are heterogeneous in resolution and contrast, reflecting real-world clinical variability.

\begin{table}[htbp]
\centering
\caption{Dataset Overview}
\label{tab:dataset}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Attribute} & \textbf{Value} \\
\midrule
Total Images & $\sim$1,200 \\
Image Format & JPEG/PNG \\
Annotation Format & YOLO TXT \\
Number of Classes & 3 \\
Average Objects/Image & 1.2 \\
Image Resolution & Variable (resized to 640$\times$640) \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Class Definitions}

The dataset includes three pathological classes as shown in Table \ref{tab:class_distribution}:

\begin{itemize}
    \item \textbf{CCT (Cerebral Cortex Tumor):} Tumorous masses originating in or affecting the cerebral cortex, typically appearing as enhanced regions on contrast MRI. CCT comprises 35.2\% of annotations.
    
    \item \textbf{IFC (Intracerebral Fluid Collection):} Abnormal accumulations of cerebrospinal fluid within the brain parenchyma. IFC represents 32.8\% of the dataset.
    
    \item \textbf{UAS (Unidentified Anomaly Signature):} Other anomalies requiring clinical attention, comprising 32.0\% of annotations.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Class Distribution showing balanced representation across pathology types}
\label{tab:class_distribution}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Instances} & \textbf{Percentage} & \textbf{Description} \\
\midrule
CCT & 423 & 35.2\% & Cerebral Cortex Tumor \\
IFC & 394 & 32.8\% & Intracerebral Fluid Collection \\
UAS & 384 & 32.0\% & Unidentified Anomaly Signature \\
\midrule
\textbf{Total} & \textbf{1,201} & \textbf{100\%} & \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Data Splitting}

Table \ref{tab:data_split} shows the dataset partitioning strategy. The 80/10/10 split ensures sufficient training data while maintaining reliable validation and test sets for evaluation.

\begin{table}[htbp]
\centering
\caption{Data Split Configuration for training, validation, and testing}
\label{tab:data_split}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Split} & \textbf{Ratio} & \textbf{Images} & \textbf{Purpose} \\
\midrule
Train & 80\% & $\sim$960 & Model training \\
Validation & 10\% & $\sim$120 & Hyperparameter tuning \\
Test & 10\% & $\sim$120 & Final evaluation \\
\bottomrule
\end{tabular}
\end{table}

For semi-supervised experiments, the training set was further divided into labeled (20\%, $\sim$192 images) and unlabeled (80\%, $\sim$768 images with labels withheld) subsets.

\subsection{Supervised Learning Baseline}

\subsubsection{YOLO Architecture Family}

Three state-of-the-art YOLO architectures were evaluated for baseline experiments. The YOLO (You Only Look Once) family represents single-stage detectors that predict bounding boxes and class probabilities directly from full images in a single network pass \citep{redmon2016yolo}.

\paragraph{YOLOv10n}
YOLOv10 introduced consistent dual assignments for NMS-free training and efficiency-accuracy driven model design \citep{wang2024yolov10}. The nano variant provides a lightweight architecture suitable for resource-constrained deployments (2.3M parameters).

\paragraph{YOLOv11n}
YOLOv11 improved upon YOLOv10 with enhanced feature aggregation, refined anchor-free detection heads, and improved CSPNet backbone modifications for better gradient flow and training stability.

\paragraph{YOLOv12n}
YOLOv12 represents the latest evolution with attention-augmented detection heads and improved neck architecture. It incorporates transformer-like attention mechanisms while maintaining YOLO's efficiency.

Figure \ref{fig:baseline_comparison} visualizes the performance comparison across these architectures.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{assignment1/model_comparison.png}
    \caption{Baseline model comparison showing mAP@0.5, mAP@0.5:0.95, Precision, Recall, and F1-score for YOLOv10n, YOLOv11n, and YOLOv12n. YOLOv12n achieves the highest performance across all metrics, demonstrating the benefits of attention mechanisms in medical imaging detection.}
    \label{fig:baseline_comparison}
\end{figure}

\subsection{Semi-Supervised Object Detection}

\subsubsection{Pseudo-Labeling Framework}

The semi-supervised pipeline employs a teacher-student framework \citep{sohn2020simple} with the following stages:

\begin{algorithm}[htbp]
\caption{Pseudo-Labeling for Semi-Supervised Object Detection}
\label{alg:pseudolabel}
\begin{algorithmic}[1]
\Require Labeled set $D_L$, Unlabeled set $D_U$, Confidence threshold $\tau$
\State \textbf{Stage 1: Train Teacher Model}
\State Train YOLOv12 on $D_L$ for 100 epochs $\rightarrow$ Teacher weights $W_T$
\State \textbf{Stage 2: Generate Pseudo-Labels}
\For{each image $x \in D_U$}
    \State $\hat{y} \leftarrow \text{Teacher}(x)$
    \If{$\text{confidence}(\hat{y}) \geq \tau$}
        \State Add $(x, \hat{y})$ to pseudo-labeled set $D_P$
    \EndIf
\EndFor
\State \textbf{Stage 3: Train Student Model}
\State Train YOLOv12 on $D_L \cup D_P$ for 100 epochs $\rightarrow$ Student weights $W_S$
\State \Return $W_S$
\end{algorithmic}
\end{algorithm}

\subsubsection{Key Hyperparameters}

Table \ref{tab:ssl_config} summarizes the semi-supervised learning configuration parameters.

\begin{table}[htbp]
\centering
\caption{Semi-Supervised Learning Configuration Parameters}
\label{tab:ssl_config}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Base Model & YOLOv12 \\
Labeled Data Ratio & 20\% \\
Confidence Threshold ($\tau$) & 0.70 \\
Teacher Training Epochs & 100 \\
Student Training Epochs & 100 \\
Batch Size & 16 \\
Learning Rate & 0.01 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Self-Supervised Learning Methods}

\subsubsection{SimCLR: Contrastive Learning}

SimCLR \citep{chen2020simclr} learns visual representations by maximizing agreement between differently augmented views of the same image. The architecture consists of a ResNet-18 encoder and a 2-layer MLP projection head (2048 $\rightarrow$ 256 $\rightarrow$ 128), producing 128-dimensional embeddings.

The Normalized Temperature-scaled Cross Entropy (NT-Xent) loss for a positive pair $(i, j)$ is:

\begin{equation}
\mathcal{L}_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbf{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}
\end{equation}

where $\text{sim}(u, v) = u^\top v / (\|u\| \|v\|)$ is cosine similarity and $\tau=0.07$ is the temperature parameter.

Figure \ref{fig:simclr_augmentation} illustrates the augmentation pipeline including random resized crop (scale: 0.2--1.0), horizontal flip, color jittering, random grayscale (p=0.2), and Gaussian blur.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{assignment2/simclr/simclr_augmentation_pairs.png}
    \caption{SimCLR augmentation pairs showing different views of the same brain MRI image. The contrastive learning objective maximizes agreement between positive pairs (same image, different augmentations) while minimizing agreement with other images in the batch.}
    \label{fig:simclr_augmentation}
\end{figure}

\subsubsection{DINOv3: Self-Distillation with Vision Transformers}

DINOv3 \citep{oquab2023dinov2} employs self-distillation with Vision Transformers (ViT-B/16, 86M parameters), learning powerful visual representations without labels from 1.7 billion diverse images. Features are extracted using the [CLS] token representation from the final transformer layer:

\begin{equation}
\mathbf{f} = \text{DINOv3}(x)_{\text{[CLS]}} \in \mathbb{R}^{768}
\end{equation}

Three downstream classifiers were evaluated: (1) Linear classifier on frozen features, (2) k-NN classifier ($k=5$), and (3) MLP classifier (768 $\rightarrow$ 256 $\rightarrow$ 128 $\rightarrow$ 3). Figure \ref{fig:dino_tsne} visualizes the natural clustering of brain MRI classes in DINOv3's learned feature space using t-SNE dimensionality reduction.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{assignment2/dinov3/dinov3_tsne_visualization.png}
    \caption{t-SNE visualization of DINOv3 features showing natural clustering of CCT (blue), IFC (orange), and UAS (green) classes. Clear separation indicates that pretrained DINOv3 features capture semantically meaningful distinctions in brain MRI pathologies without any task-specific training.}
    \label{fig:dino_tsne}
\end{figure}

\subsection{YOLO Integration for Object Detection}

Both self-supervised methods were integrated with YOLOv12 for object detection through feature-enhanced training, where the pretrained representations enhance detection through improved feature initialization and classification-guided confidence estimation.

\subsection{Evaluation Metrics}

Performance was evaluated using: \textbf{mAP@0.5} (Mean Average Precision at IoU=0.5), \textbf{mAP@0.5:0.95} (averaged over IoU thresholds 0.5--0.95), \textbf{Precision} (true positives / all predictions), \textbf{Recall} (true positives / actual positives), and \textbf{F1-Score} (harmonic mean of precision and recall).
